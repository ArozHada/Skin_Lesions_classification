{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CADx_Challenge2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Classification of Skin Lesions : HAM1000 Dataset\n",
        "CADx_Challenge\n",
        "--------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "y4E1T1SopdA7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YfcsLN7oBMO"
      },
      "source": [
        "Mount the drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBwFJ-22tLR1",
        "outputId": "95ed510c-7d70-4219-ec12-ba27391109f6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnQObbt9oAg4"
      },
      "source": [
        "Installing a newer version for implementing sift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djxx23istwxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28316c2f-a60b-4957-94a0-5b34fc95e911"
      },
      "source": [
        "!pip install opencv-contrib-python==4.4.0.44"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-contrib-python==4.4.0.44\n",
            "  Downloading opencv_contrib_python-4.4.0.44-cp37-cp37m-manylinux2014_x86_64.whl (55.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 55.7 MB 60 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==4.4.0.44) (1.19.5)\n",
            "Installing collected packages: opencv-contrib-python\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.1.2.30\n",
            "    Uninstalling opencv-contrib-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-contrib-python-4.1.2.30\n",
            "Successfully installed opencv-contrib-python-4.4.0.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8yxRzr3u4r8"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from skimage.feature import local_binary_pattern\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, \\\n",
        "                            recall_score, accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxSS09gip0dV"
      },
      "source": [
        "Extract Features \n",
        "1. Hu moments\n",
        "2. LBP\n",
        "3. color histogram\n",
        "\n",
        "Features Tried But did not work well\n",
        "4. HoG [Decrease in Accuracy]\n",
        "5. GLCM [Decrease in Accuracy]\n",
        "6. Haralick [Could not start mahotas.features ]\n",
        "\n",
        "Global Features\n",
        "7. SIFT [Could not combine local and global features using K-means]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUT0I3WFsWW0"
      },
      "source": [
        "def extract_hu_moments(img):\n",
        "    \"\"\"Extract Hu Moments feature of an image. Hu Moments are shape descriptors.\n",
        "    :param img: ndarray, BGR image\n",
        "    :return feature: ndarray, contains 7 Hu Moments of the image\n",
        "    \"\"\"\n",
        "    \n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    feature = cv2.HuMoments(cv2.moments(gray)).flatten()\n",
        "    return feature\n",
        "\n",
        "def extract_lbp(img, numPoints=24, radius=8):\n",
        "    \"\"\"Extract Local Binary Pattern histogram of an image. Local Binary Pattern features are texture descriptors.\n",
        "    :param img: ndarray, BGR image\n",
        "    :return feature: ndarray, contains (numPoints+2) Local Binary Pattern histogram of the image\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    lbp = local_binary_pattern(gray, numPoints, radius, method='uniform')\n",
        "    n_bins = int(lbp.max() + 1)\n",
        "    feature, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
        "    return feature\n",
        "\n",
        "\n",
        "def extract_color_histogram(img, n_bins=8):\n",
        "    \"\"\"Extract Color histogram of an image.\n",
        "    :param img: ndarray, BGR image\n",
        "    :return feature: ndarray, contains n_bins*n_bins*n_bins HSV histogram features of the image\n",
        "    \"\"\"\n",
        "    \n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # convert the image to HSV color-space\n",
        "    hist  = cv2.calcHist([hsv], [0, 1, 2], None, [n_bins, n_bins, n_bins], [0, 180, 0, 256, 0, 256])\n",
        "    cv2.normalize(hist, hist)\n",
        "    feature = hist.flatten()\n",
        "    return feature\n",
        "\n",
        "##Extracting the global features\n",
        "def extract_global_features(img):\n",
        "    \"\"\"Extract global features (shape, texture and color features) of an image.\n",
        "    :param img: ndarray, BGR image\n",
        "    :return global_feature: ndarray, contains shape, texture and color features of the image\n",
        "    \"\"\"\n",
        "    \n",
        "    hu_moments = extract_hu_moments(img)\n",
        "    #zernike_moments = extract_zernike_moments(img)\n",
        "    #haralick   = extract_haralick(img)\n",
        "    lbp_histogram  = extract_lbp(img)\n",
        "    color_histogram  = extract_color_histogram(img)\n",
        "    global_feature = np.hstack([hu_moments, lbp_histogram, color_histogram])\n",
        "    \n",
        "    return global_feature\n",
        "\n",
        "#### Attempt at extracting the local features but not successfull \n",
        "\n",
        "# def extract_keypoints(keypoint_detector, image):    \n",
        "#     keypoints, descriptors = keypoint_detector.detectAndCompute(image, None)\n",
        "#     return np.array(descriptors) if descriptors is not None else np.array([])\n",
        "\n",
        "\n",
        "# def flatten_keypoint_descriptors(X_train_local_features):\n",
        "#     descriptor_list_train = np.array(X_train_local_features[0])\n",
        "#     for remaining in X_train_local_features[1:]:\n",
        "#     \tdescriptor_list_train = np.vstack((descriptor_list_train, remaining))\n",
        "#     return descriptor_list_train\n",
        "\n",
        "\n",
        "# def cluster_local_features(descriptor_list_train, n_clusters=20): \n",
        "#     kmeans = KMeans(n_clusters=n_clusters)\n",
        "#     kmeans.fit(descriptor_list_train)\n",
        "#     return kmeans\n",
        "    \n",
        "\n",
        "# def extract_local_features(X_train_local_features, X_test_local_features):\n",
        "    \n",
        "#     k=20\n",
        "\n",
        "#     # # Scaling descriptors\n",
        "#     # scaler = MinMaxScaler()\n",
        "#     # X_train_local_features = scaler.fit_transform(X_train_local_features)\n",
        "#     # X_test_local_features = scaler.fit_transform(X_test_local_features)\n",
        "    \n",
        "#     km = KMeans(n_clusters=k)\n",
        "#     kmeans = km.fit(X_train_local_features)\n",
        "    \n",
        "#     print(\"[INFO] Adding cluster features\")\n",
        "#     X_clustered_train = kmeans.predict(X_train_local_features)\n",
        "#     X_clustered_test  = kmeans.predict(X_test_local_features)\n",
        "\n",
        "#     return X_clustered_train, X_clustered_test\n",
        "\n",
        "#### 2nd Attempt at extracting the local features but did not work \n",
        "\n",
        "# def extract_local_features(X_train_local_features, X_test_local_features):\n",
        "    \n",
        "#     n_clusters=20\n",
        "\n",
        "#     # flatten keypoint_descriptors\n",
        "#     # descriptor_list_train = flatten_keypoint_descriptors(X_train_local_features)\n",
        "#     # descriptor_list_test = flatten_keypoint_descriptors(X_test_local_features)\n",
        " \n",
        "#     # cluster keypoint descriptors\n",
        "#     kmeans = cluster_local_features(X_train_local_features, n_clusters=n_clusters)\n",
        "#     descriptor_clustered_train = kmeans.predict(X_train_local_features)\n",
        "#     descriptor_clustered_test = kmeans.predict(X_train_local_features)\n",
        "\n",
        "#     # For each image, count number of keypoints in each cluster that the image has\n",
        "#     X_clustered_train = np.array([np.zeros(n_clusters) for i in range(len(X_train_local_features))])\n",
        "#     old_count = 0\n",
        "#     for i in range(len(X_train_local_features)):\n",
        "#     \tnb_descriptors = len(X_train_local_features[i])\n",
        "#     \tfor j in range(nb_descriptors):\n",
        "#     \t\tidx = descriptor_clustered_train[old_count+j]\n",
        "#     \t\tX_clustered_train[i][idx] += 1\n",
        "#     \told_count += nb_descriptors\n",
        "    \n",
        "#     X_clustered_test = np.array([np.zeros(n_clusters) for i in range(len(X_test_local_features))])\n",
        "#     old_count = 0\n",
        "#     for i in range(len(X_test_local_features)):\n",
        "#         nb_descriptors = len(X_test_local_features[i])\n",
        "#         for j in range(nb_descriptors):\n",
        "#             idx = descriptor_clustered_test[old_count+j]\n",
        "#             X_clustered_test[i][idx] += 1\n",
        "#         old_count += nb_descriptors\n",
        "        \n",
        "#     return X_clustered_train, X_clustered_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSGcAsxaqkUU"
      },
      "source": [
        "Fitting the PCA algorithm with our extended features did not give promising results, therefore not using in final version of classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI0uToxFqjc9"
      },
      "source": [
        "#### Fitting the PCA algorithm \n",
        "\n",
        "# def show_variance_explained_pca(data):\n",
        "#     \"\"\"\n",
        "#     Showing cumulative variance explained by the principle components after performing pca on input dataframe\n",
        "#     \"\"\"\n",
        "#     pca = PCA().fit(data)\n",
        "#     #Plotting the Cumulative Summation of the Explained Variance\n",
        "#     plt.figure()\n",
        "#     plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "#     plt.xlabel('Number of Components')\n",
        "#     plt.ylabel('Variance (%)') #for each component\n",
        "#     plt.title('Explained Variance')\n",
        "#     plt.show()\n",
        "    \n",
        "# def fit_pca(pca, train_x, val_x):\n",
        "#     \"\"\"\n",
        "#     Returning the transformed datasets using the provided pca\n",
        "#     Fitting using only training data, transforms both\n",
        "#     \"\"\"\n",
        "#     train_x_pca = pca.fit_transform(train_x)\n",
        "#     val_x_pca = pca.transform(val_x)    \n",
        "#     return train_x_pca, test_x_pca\n",
        "\n",
        "\n",
        "# pca = PCA().fit(train_data)\n",
        "# plt.figure()\n",
        "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "# plt.xlabel('Number of Components')\n",
        "# plt.ylabel('Variance (%)') #for each component\n",
        "# plt.title('Explained Variance')\n",
        "# plt.show()\n",
        "# train_x_pca = pca.fit_transform(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eINw3s_ZvKQ7"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, os.getcwd()) # add current working directory to pythonpath\n",
        "\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import warnings\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu1JRZulvO2d"
      },
      "source": [
        "def resize_image(img, img_size):\n",
        "    \"\"\"Pad and Resize image to a fixed size (img_size, img_size)\n",
        "    :param img: ndarray, BGR image\n",
        "    :param img_size: int, size to resize images.\n",
        "    :return img_resized: padded and resized image\n",
        "    \"\"\"\n",
        "    \n",
        "    # pad image to the max dimension\n",
        "    top = (max(img.shape[:2]) - img.shape[0])//2\n",
        "    bottom = max(img.shape[:2]) - img.shape[0] - top\n",
        "    left = (max(img.shape[:2]) - img.shape[1])//2\n",
        "    right = max(img.shape[:2]) - img.shape[1] - left\n",
        "    img_padded = cv2.copyMakeBorder(img, top, bottom, left, right,cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    \n",
        "    # resize image\n",
        "    img_resized = cv2.resize(img_padded, (img_size, img_size))\n",
        "    return img_resized\n",
        "\n",
        "#### Removing hair from the provided images using blackhat\n",
        "def remove_hair(image):\n",
        "  # Convert the original image to grayscale\n",
        "  grayScale = cv2.cvtColor( image, cv2.COLOR_RGB2GRAY ) \n",
        "  #cv2_imshow(grayScale)\n",
        "\n",
        "  # Kernel for the morphological filtering\n",
        "  kernel = cv2.getStructuringElement(1,(17,17))\n",
        "\n",
        "  # clahe = cv2.createCLAHE(clipLimit = 5)\n",
        "  # clahe_img = clahe.apply(grayScale) + 30\n",
        "\n",
        "  # Perform the blackHat filtering on the grayscale image to find the hair countours\n",
        "  blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
        "  #cv2_imshow(blackhat)\n",
        "\n",
        "  # intensify the hair countours in preparation for the inpainting  \n",
        "  # algorithm \n",
        "  ret,thresh2 = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n",
        "  #cv2_imshow(thresh2)\n",
        "\n",
        "  # inpaint the original image depending on the mask\n",
        "  dst = cv2.inpaint(image,thresh2,1,cv2.INPAINT_TELEA)\n",
        "  #cv2_imshow(dst)\n",
        "\n",
        "  return dst\n",
        "\n",
        "#### Cycle through the folders, get labels and read each image, resize > remove hair > extract local and global features \n",
        "\n",
        "    \n",
        "def prepare_dataset(input_path, img_size=None, keypoint_detector=None):\n",
        "    \"\"\"Process images of different classes and extract labels and global features of images\n",
        "    :param input_path: str, path to the a dataset folder, which should have the below structure:\n",
        "        input_path\n",
        "            |---class_1\n",
        "            |---class_2\n",
        "            ...\n",
        "            |---class_N\n",
        "    : img_size: int, size to resize images. If None, resize will not be performed.\n",
        "    :return global_features: list, contains global features of images\n",
        "    :return label: list, contains labels (or classes) of images\n",
        "    \"\"\"\n",
        "    \n",
        "    # if keypoint_detector is None:\n",
        "    #     keypoint_detector = cv2.SIFT_create()\n",
        " \n",
        "    global_features = []\n",
        "    local_features = []\n",
        "    labels = []\n",
        "    folder_list = os.listdir(input_path)\n",
        "    for folder in folder_list:\n",
        "        print('Processing: ' + folder)       \n",
        "        folder_path = os.path.join(input_path, folder)\n",
        "        file_list = os.listdir(folder_path)\n",
        "        for filename in file_list:\n",
        "            img = cv2.imread(os.path.join(folder_path, filename))   #[:, :, :3] # ignore alpha channel\n",
        "            if img_size is not None:\n",
        "                img = resize_image(img, img_size)\n",
        "                img = remove_hair(img)\n",
        "            global_feature = extract_global_features(img)\n",
        "            global_features.append(global_feature)\n",
        "            labels.append(folder)\n",
        "            \n",
        "            #keypoint_descriptors = extract_keypoints(keypoint_detector, img)\n",
        "            #local_features.append(keypoint_descriptors)\n",
        "                   \n",
        "    return np.array(global_features), local_features, np.array(labels) \n",
        "    \n",
        "\n",
        "def train_model(model, X_train, y_train, parameters, n_splits=3):\n",
        "    \"\"\"Train model with Grid-search cross validation to find the best hyperparameter\n",
        "    :param model: Scikit-learn estimator\n",
        "    :param X_train: trainset features\n",
        "    :param y_train: trainset label\n",
        "    :param parameters: dict, key is hyper parameter name and value is a list of hyper parameter values\n",
        "    :return best_estimator: Scikit-learn estimator with the best hyper parameter\n",
        "    :return best_score: best accuracy score\n",
        "    :return best_param: dict, best hyper parameter\n",
        "    \"\"\"\n",
        "    \n",
        "    splits = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0).split(X=X_train, y=y_train)\n",
        "    \n",
        "    clf = GridSearchCV(model, parameters, cv=splits, scoring=make_scorer(accuracy_score))\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')  # disable the warning on default optimizer\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "    return clf.best_estimator_, clf.best_score_, clf.best_params_\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Evaluate model on testset\n",
        "    :param model: Scikit-learn estimator\n",
        "    :param X_train: trainset features\n",
        "    :param y_train: trainset label\n",
        "    :param X_test: testset features\n",
        "    :param y_test: testset label\n",
        "    :param parameters: dict, key is hyper parameter name and value is a list of hyper parameter values\n",
        "    :return model: Scikit-learn estimator, fitted on the whole trainset\n",
        "    :return y_predict: prediction on test set\n",
        "    :return scores: dict, evaluation metrics on test set\n",
        "    \"\"\"\n",
        "    \n",
        "    # Refit the model on the whole train set\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')  # disable the warning on default optimizer\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "    # Evaluate on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    scores = None\n",
        "    if y_test is not None:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')  # disable the warning on f1-score with not all labels\n",
        "            #scores = get_prediction_score(y_test, y_predict)\n",
        "            print('*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***')\n",
        "            # compute and plot performance metrics\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            scores = accuracy\n",
        "            val_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "            val_recall = recall_score(y_test, y_pred, average='weighted')\n",
        "            val_precision = precision_score(y_test, y_pred, average='weighted')\n",
        "            val_kappa = cohen_kappa_score(y_test,y_pred,weights='quadratic')\n",
        "\n",
        "            print('Accuracy: {:.3f}'.format(accuracy))\n",
        "            print('Kappa: {:.3f}'.format(val_kappa))\n",
        "            print('F1-score: {:.3f}'.format(val_f1))\n",
        "            print('Recall: {:.3f}'.format(val_recall))\n",
        "            print('Precision: {:.3f}'.format(val_precision))\n",
        "    return model, y_pred, scores\n",
        "\n",
        "\n",
        "def build_base_models(X_train, y_train):\n",
        "    \"\"\"Train and evaluate different base models. \"Base\" means the model is not a stacking model. \n",
        "    :param X_train: trainset features\n",
        "    :param y_train: trainset label\n",
        "    :return fitted_models: list, contains fitted Scikit-learn estimators\n",
        "    :return model_names: list, names of fitted Scikit-learn estimators\n",
        "    :return model_scores: list, contains scores on test set for fitted Scikit-learn estimators.\n",
        "                    Each score is a dict of evaluation metrics\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    #### DEFINE BASE MODELS \n",
        "\n",
        "    models = []\n",
        "    model_params = []\n",
        "    model_names = []\n",
        "    \n",
        "    # Random forest model\n",
        "    for n_estimators in [500, 1000, 2000]:\n",
        "        for max_depth in [3, 5, 7]:\n",
        "            models.append(RandomForestClassifier(max_features='sqrt', class_weight='balanced', random_state=0))\n",
        "            model_params.append({'n_estimators':[n_estimators], 'max_depth':[max_depth]})\n",
        "            model_names.append('Random Forest')   \n",
        "    \n",
        "    # Boosted Tree\n",
        "    for n_estimators in [500, 1000, 2000]:\n",
        "        for max_depth in [3, 5, 7]:\n",
        "            for learning_rate in [0.01, 0.1]:\n",
        "                models.append(GradientBoostingClassifier(subsample=0.7, max_features='sqrt', random_state=0))\n",
        "                model_params.append({'n_estimators':[n_estimators], 'max_depth':[max_depth], 'learning_rate':[learning_rate]})\n",
        "                model_names.append('Gradient Boosting Machine')\n",
        "    \n",
        "    # SVM\n",
        "    for kernel in ['linear', 'rbf']:\n",
        "        for C in [1.0, 10.0, 100.0, 1000.0]:\n",
        "            models.append(SVC(probability=True, gamma='auto', tol=0.001, cache_size=200, class_weight='balanced',\n",
        "                              random_state=0,\n",
        "                              decision_function_shape='ovr'))\n",
        "            model_params.append({'kernel':[kernel], 'C':[C]})\n",
        "            model_names.append('Support Vector Machine')\n",
        "    \n",
        "    # Logistic regression model\n",
        "    for penalty in ['l1', 'l2']:\n",
        "        for C in [1.0, 10.0, 100.0, 1000.0]:\n",
        "            models.append(linear_model.LogisticRegression(max_iter=500, solver='liblinear', multi_class='ovr',\n",
        "                                                          class_weight='balanced', random_state=0))\n",
        "            model_params.append({'penalty':[penalty], 'C':[C]})\n",
        "            model_names.append('Logistic Regression')\n",
        "        \n",
        "    # KNN\n",
        "    for n_neighbors in [5, 10, 15]:\n",
        "        for weights in ['uniform', 'distance']:\n",
        "            models.append(KNeighborsClassifier())\n",
        "            model_params.append({'n_neighbors':[n_neighbors], 'weights':[weights]})\n",
        "            model_names.append('K Nearest Neighbour')\n",
        "            \n",
        "\n",
        "    #### TRAIN AND EVALUATE MODELS #\n",
        "\n",
        "    fitted_models = []\n",
        "    model_scores = []\n",
        "    for i in range(len(models)):\n",
        "        print('Evaluating model {} of {}: {}'.format((i+1), len(models), model_names[i]))\n",
        "        model = models[i]\n",
        "        fitted_cv, _, _ = train_model(model=model, X_train=X_train, y_train=y_train, parameters=model_params[i])\n",
        "        fitted_whole_set, _, score = evaluate_model(model=fitted_cv, X_train=X_train, y_train=y_train,\n",
        "                                                    X_test=X_test, y_test=y_test)\n",
        "        fitted_models.append(fitted_whole_set)\n",
        "        model_scores.append(score)\n",
        "        print(model_names[i], score)\n",
        "        \n",
        "    return fitted_models, model_names, model_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxrGRDnXtS0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96755cd-d1de-4ca2-fe18-a92a93a0a680"
      },
      "source": [
        "#Path for the data and output \n",
        "data_path = \"/drive/MyDrive/cad/cad2\"\n",
        "save_path = \"/drive/MyDrive/cad/cad2/output2\"\n",
        "\n",
        "#Variables \n",
        "img_size = None\n",
        "n_splits = 3\n",
        "\n",
        "# Extract features and labels for train set and test set\n",
        "train_path = \"/drive/MyDrive/CADx project /Challenges/cad/cad2/train1/train\"\n",
        "test_path = \"/drive/MyDrive/CADx project /Challenges/cad/cad2/val1/val\"\n",
        "\n",
        "#Extract global features for train and validation set \n",
        "X_train_global_features, keypoints_features_train,y_train = prepare_dataset(train_path, img_size=img_size)\n",
        "X_test_global_features, keypoints_features_test,y_test = prepare_dataset(test_path, img_size=img_size)\n",
        "\n",
        "#### Attempt at clustering the local features and combining them with global features \n",
        "#X_clustered_train, X_clustered_test = extract_local_features(keypoints_features_train,keypoints_features_test)\n",
        "# X_train_all_features = np.hstack((X_train_global_features, X_clustered_train))\n",
        "# X_test_all_features = np.hstack((X_test_global_features, X_clustered_test))\n",
        "\n",
        "#Change of names \n",
        "X_train_all_features = X_train_global_features\n",
        "X_test_all_features = X_test_global_features\n",
        "\n",
        "# Normalize features\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train = scaler.fit_transform(X_train_all_features)\n",
        "X_test = scaler.transform(X_test_all_features)\n",
        "\n",
        "# Build base models\n",
        "base_models, base_model_names, base_model_scores = build_base_models(X_train, y_train)\n",
        "if save_path is not None:\n",
        "    # Save base models    \n",
        "    os.makedirs(os.path.join(save_path, 'base_models'), exist_ok=True)\n",
        "    for i in range(len(base_models)):\n",
        "        with open(os.path.join(save_path, 'base_models', 'base_model_' + str(i+1) + '.pkl'), 'wb') as f:\n",
        "            pickle.dump(base_models[i], f)\n",
        "                    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: bkl\n",
            "Processing: bcc\n",
            "Processing: mel\n",
            "Processing: bkl\n",
            "Processing: mel\n",
            "Processing: bcc\n",
            "Evaluating model 1 of 49: Random Forest\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.628\n",
            "Kappa: 0.556\n",
            "F1-score: 0.623\n",
            "Recall: 0.628\n",
            "Precision: 0.642\n",
            "Random Forest 0.628\n",
            "Evaluating model 2 of 49: Random Forest\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.670\n",
            "Kappa: 0.613\n",
            "F1-score: 0.668\n",
            "Recall: 0.670\n",
            "Precision: 0.674\n",
            "Random Forest 0.67\n",
            "Evaluating model 3 of 49: Random Forest\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.692\n",
            "Kappa: 0.632\n",
            "F1-score: 0.691\n",
            "Recall: 0.692\n",
            "Precision: 0.694\n",
            "Random Forest 0.692\n",
            "Evaluating model 4 of 49: Random Forest\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.614\n",
            "Kappa: 0.541\n",
            "F1-score: 0.611\n",
            "Recall: 0.614\n",
            "Precision: 0.626\n",
            "Random Forest 0.614\n",
            "Evaluating model 5 of 49: Random Forest\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.672\n",
            "Kappa: 0.612\n",
            "F1-score: 0.670\n",
            "Recall: 0.672\n",
            "Precision: 0.676\n",
            "Random Forest 0.672\n",
            "Evaluating model 6 of 49: Random Forest\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.694\n",
            "Kappa: 0.633\n",
            "F1-score: 0.692\n",
            "Recall: 0.694\n",
            "Precision: 0.697\n",
            "Random Forest 0.694\n",
            "Evaluating model 7 of 49: Random Forest\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.616\n",
            "Kappa: 0.540\n",
            "F1-score: 0.612\n",
            "Recall: 0.616\n",
            "Precision: 0.630\n",
            "Random Forest 0.616\n",
            "Evaluating model 8 of 49: Random Forest\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.666\n",
            "Kappa: 0.604\n",
            "F1-score: 0.664\n",
            "Recall: 0.666\n",
            "Precision: 0.671\n",
            "Random Forest 0.666\n",
            "Evaluating model 9 of 49: Random Forest\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.694\n",
            "Kappa: 0.637\n",
            "F1-score: 0.692\n",
            "Recall: 0.694\n",
            "Precision: 0.697\n",
            "Random Forest 0.694\n",
            "Evaluating model 10 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.690\n",
            "Kappa: 0.620\n",
            "F1-score: 0.689\n",
            "Recall: 0.690\n",
            "Precision: 0.696\n",
            "Gradient Boosting Machine 0.69\n",
            "Evaluating model 11 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.730\n",
            "Kappa: 0.701\n",
            "F1-score: 0.730\n",
            "Recall: 0.730\n",
            "Precision: 0.734\n",
            "Gradient Boosting Machine 0.73\n",
            "Evaluating model 12 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.712\n",
            "Kappa: 0.665\n",
            "F1-score: 0.710\n",
            "Recall: 0.712\n",
            "Precision: 0.718\n",
            "Gradient Boosting Machine 0.712\n",
            "Evaluating model 13 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.746\n",
            "Kappa: 0.720\n",
            "F1-score: 0.746\n",
            "Recall: 0.746\n",
            "Precision: 0.750\n",
            "Gradient Boosting Machine 0.746\n",
            "Evaluating model 14 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.718\n",
            "Kappa: 0.652\n",
            "F1-score: 0.715\n",
            "Recall: 0.718\n",
            "Precision: 0.724\n",
            "Gradient Boosting Machine 0.718\n",
            "Evaluating model 15 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.746\n",
            "Kappa: 0.707\n",
            "F1-score: 0.746\n",
            "Recall: 0.746\n",
            "Precision: 0.748\n",
            "Gradient Boosting Machine 0.746\n",
            "Evaluating model 16 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.712\n",
            "Kappa: 0.669\n",
            "F1-score: 0.712\n",
            "Recall: 0.712\n",
            "Precision: 0.716\n",
            "Gradient Boosting Machine 0.712\n",
            "Evaluating model 17 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.750\n",
            "Kappa: 0.736\n",
            "F1-score: 0.751\n",
            "Recall: 0.750\n",
            "Precision: 0.754\n",
            "Gradient Boosting Machine 0.75\n",
            "Evaluating model 18 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.730\n",
            "Kappa: 0.701\n",
            "F1-score: 0.730\n",
            "Recall: 0.730\n",
            "Precision: 0.735\n",
            "Gradient Boosting Machine 0.73\n",
            "Evaluating model 19 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.752\n",
            "Kappa: 0.727\n",
            "F1-score: 0.752\n",
            "Recall: 0.752\n",
            "Precision: 0.755\n",
            "Gradient Boosting Machine 0.752\n",
            "Evaluating model 20 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.728\n",
            "Kappa: 0.670\n",
            "F1-score: 0.726\n",
            "Recall: 0.728\n",
            "Precision: 0.733\n",
            "Gradient Boosting Machine 0.728\n",
            "Evaluating model 21 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.744\n",
            "Kappa: 0.705\n",
            "F1-score: 0.744\n",
            "Recall: 0.744\n",
            "Precision: 0.746\n",
            "Gradient Boosting Machine 0.744\n",
            "Evaluating model 22 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.734\n",
            "Kappa: 0.711\n",
            "F1-score: 0.734\n",
            "Recall: 0.734\n",
            "Precision: 0.736\n",
            "Gradient Boosting Machine 0.734\n",
            "Evaluating model 23 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.744\n",
            "Kappa: 0.722\n",
            "F1-score: 0.744\n",
            "Recall: 0.744\n",
            "Precision: 0.746\n",
            "Gradient Boosting Machine 0.744\n",
            "Evaluating model 24 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.746\n",
            "Kappa: 0.718\n",
            "F1-score: 0.745\n",
            "Recall: 0.746\n",
            "Precision: 0.751\n",
            "Gradient Boosting Machine 0.746\n",
            "Evaluating model 25 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.750\n",
            "Kappa: 0.725\n",
            "F1-score: 0.750\n",
            "Recall: 0.750\n",
            "Precision: 0.753\n",
            "Gradient Boosting Machine 0.75\n",
            "Evaluating model 26 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.746\n",
            "Kappa: 0.697\n",
            "F1-score: 0.745\n",
            "Recall: 0.746\n",
            "Precision: 0.750\n",
            "Gradient Boosting Machine 0.746\n",
            "Evaluating model 27 of 49: Gradient Boosting Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.744\n",
            "Kappa: 0.705\n",
            "F1-score: 0.744\n",
            "Recall: 0.744\n",
            "Precision: 0.746\n",
            "Gradient Boosting Machine 0.744\n",
            "Evaluating model 28 of 49: Support Vector Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.600\n",
            "Kappa: 0.522\n",
            "F1-score: 0.595\n",
            "Recall: 0.600\n",
            "Precision: 0.625\n",
            "Support Vector Machine 0.6\n",
            "Evaluating model 29 of 49: Support Vector Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.620\n",
            "Kappa: 0.563\n",
            "F1-score: 0.617\n",
            "Recall: 0.620\n",
            "Precision: 0.645\n",
            "Support Vector Machine 0.62\n",
            "Evaluating model 30 of 49: Support Vector Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.626\n",
            "Kappa: 0.523\n",
            "F1-score: 0.624\n",
            "Recall: 0.626\n",
            "Precision: 0.645\n",
            "Support Vector Machine 0.626\n",
            "Evaluating model 31 of 49: Support Vector Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.606\n",
            "Kappa: 0.483\n",
            "F1-score: 0.606\n",
            "Recall: 0.606\n",
            "Precision: 0.633\n",
            "Support Vector Machine 0.606\n",
            "Evaluating model 32 of 49: Support Vector Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.518\n",
            "Kappa: 0.307\n",
            "F1-score: 0.515\n",
            "Recall: 0.518\n",
            "Precision: 0.553\n",
            "Support Vector Machine 0.518\n",
            "Evaluating model 33 of 49: Support Vector Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.574\n",
            "Kappa: 0.399\n",
            "F1-score: 0.574\n",
            "Recall: 0.574\n",
            "Precision: 0.616\n",
            "Support Vector Machine 0.574\n",
            "Evaluating model 34 of 49: Support Vector Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.598\n",
            "Kappa: 0.508\n",
            "F1-score: 0.593\n",
            "Recall: 0.598\n",
            "Precision: 0.622\n",
            "Support Vector Machine 0.598\n",
            "Evaluating model 35 of 49: Support Vector Machine\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.618\n",
            "Kappa: 0.553\n",
            "F1-score: 0.614\n",
            "Recall: 0.618\n",
            "Precision: 0.639\n",
            "Support Vector Machine 0.618\n",
            "Evaluating model 36 of 49: Logistic Regression\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.642\n",
            "Kappa: 0.585\n",
            "F1-score: 0.639\n",
            "Recall: 0.642\n",
            "Precision: 0.652\n",
            "Logistic Regression 0.642\n",
            "Evaluating model 37 of 49: Logistic Regression\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.650\n",
            "Kappa: 0.567\n",
            "F1-score: 0.648\n",
            "Recall: 0.650\n",
            "Precision: 0.660\n",
            "Logistic Regression 0.65\n",
            "Evaluating model 38 of 49: Logistic Regression\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.634\n",
            "Kappa: 0.530\n",
            "F1-score: 0.633\n",
            "Recall: 0.634\n",
            "Precision: 0.642\n",
            "Logistic Regression 0.634\n",
            "Evaluating model 39 of 49: Logistic Regression\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.620\n",
            "Kappa: 0.518\n",
            "F1-score: 0.619\n",
            "Recall: 0.620\n",
            "Precision: 0.630\n",
            "Logistic Regression 0.62\n",
            "Evaluating model 40 of 49: Logistic Regression\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.624\n",
            "Kappa: 0.545\n",
            "F1-score: 0.621\n",
            "Recall: 0.624\n",
            "Precision: 0.637\n",
            "Logistic Regression 0.624\n",
            "Evaluating model 41 of 49: Logistic Regression\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.652\n",
            "Kappa: 0.571\n",
            "F1-score: 0.650\n",
            "Recall: 0.652\n",
            "Precision: 0.662\n",
            "Logistic Regression 0.652\n",
            "Evaluating model 42 of 49: Logistic Regression\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.638\n",
            "Kappa: 0.562\n",
            "F1-score: 0.637\n",
            "Recall: 0.638\n",
            "Precision: 0.646\n",
            "Logistic Regression 0.638\n",
            "Evaluating model 43 of 49: Logistic Regression\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.646\n",
            "Kappa: 0.550\n",
            "F1-score: 0.646\n",
            "Recall: 0.646\n",
            "Precision: 0.657\n",
            "Logistic Regression 0.646\n",
            "Evaluating model 44 of 49: K Nearest Neighbour\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.570\n",
            "Kappa: 0.367\n",
            "F1-score: 0.572\n",
            "Recall: 0.570\n",
            "Precision: 0.591\n",
            "K Nearest Neighbour 0.57\n",
            "Evaluating model 45 of 49: K Nearest Neighbour\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.592\n",
            "Kappa: 0.407\n",
            "F1-score: 0.594\n",
            "Recall: 0.592\n",
            "Precision: 0.599\n",
            "K Nearest Neighbour 0.592\n",
            "Evaluating model 46 of 49: K Nearest Neighbour\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.566\n",
            "Kappa: 0.369\n",
            "F1-score: 0.568\n",
            "Recall: 0.566\n",
            "Precision: 0.586\n",
            "K Nearest Neighbour 0.566\n",
            "Evaluating model 47 of 49: K Nearest Neighbour\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.588\n",
            "Kappa: 0.418\n",
            "F1-score: 0.589\n",
            "Recall: 0.588\n",
            "Precision: 0.595\n",
            "K Nearest Neighbour 0.588\n",
            "Evaluating model 48 of 49: K Nearest Neighbour\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.590\n",
            "Kappa: 0.405\n",
            "F1-score: 0.591\n",
            "Recall: 0.590\n",
            "Precision: 0.599\n",
            "K Nearest Neighbour 0.59\n",
            "Evaluating model 49 of 49: K Nearest Neighbour\n",
            "*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***\n",
            "Accuracy: 0.586\n",
            "Kappa: 0.379\n",
            "F1-score: 0.587\n",
            "Recall: 0.586\n",
            "Precision: 0.590\n",
            "K Nearest Neighbour 0.586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpbH8wL2izK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dbe8de3-029c-4516-b315-71f6c48a7d35"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "#Set test folder path \n",
        "test_path = \"/drive/MyDrive/CADx project /Challenges/cad/cad2/test1\"\n",
        "\n",
        "#Set variables \n",
        "img_size = None\n",
        "test_global_features = []\n",
        "\n",
        "\n",
        "#Read images through the folder and extract features from them \n",
        "file_list = os.listdir(test_path)\n",
        "sorted_files = sorted(file_list)\n",
        "print(sorted_files)\n",
        "print(len(sorted_files))\n",
        "print(len(file_list))                             # Print the number of images present for later reference in prediction file \n",
        "for filename in sorted_files:\n",
        "  img = cv2.imread(os.path.join(test_path, filename))\n",
        "  if img_size is not None:\n",
        "    img = resize_image(img, img_size)\n",
        "    img = remove_hair(img)\n",
        "  test_feature = extract_global_features(img)\n",
        "  test_global_features.append(test_feature)\n",
        "           \n",
        "X_test_global_features = np.array(test_global_features)  \n",
        "print(np.array(X_test_global_features).shape)       # print the shape of the extracted features \n",
        "print(\"Feature Extration Complete\")\n",
        "\n",
        "# Normalize features\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train = scaler.fit_transform(X_train_all_features)\n",
        "X_test = scaler.transform(X_test_global_features)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['xxx0001.jpg', 'xxx0002.jpg', 'xxx0003.jpg', 'xxx0004.jpg', 'xxx0005.jpg', 'xxx0006.jpg', 'xxx0007.jpg', 'xxx0008.jpg', 'xxx0009.jpg', 'xxx0010.jpg', 'xxx0011.jpg', 'xxx0012.jpg', 'xxx0013.jpg', 'xxx0014.jpg', 'xxx0015.jpg', 'xxx0016.jpg', 'xxx0017.jpg', 'xxx0018.jpg', 'xxx0019.jpg', 'xxx0020.jpg', 'xxx0021.jpg', 'xxx0022.jpg', 'xxx0023.jpg', 'xxx0024.jpg', 'xxx0025.jpg', 'xxx0026.jpg', 'xxx0027.jpg', 'xxx0028.jpg', 'xxx0029.jpg', 'xxx0030.jpg', 'xxx0031.jpg', 'xxx0032.jpg', 'xxx0033.jpg', 'xxx0034.jpg', 'xxx0035.jpg', 'xxx0036.jpg', 'xxx0037.jpg', 'xxx0038.jpg', 'xxx0039.jpg', 'xxx0040.jpg', 'xxx0041.jpg', 'xxx0042.jpg', 'xxx0043.jpg', 'xxx0044.jpg', 'xxx0045.jpg', 'xxx0046.jpg', 'xxx0047.jpg', 'xxx0048.jpg', 'xxx0049.jpg', 'xxx0050.jpg', 'xxx0051.jpg', 'xxx0052.jpg', 'xxx0053.jpg', 'xxx0054.jpg', 'xxx0055.jpg', 'xxx0056.jpg', 'xxx0057.jpg', 'xxx0058.jpg', 'xxx0059.jpg', 'xxx0060.jpg', 'xxx0061.jpg', 'xxx0062.jpg', 'xxx0063.jpg', 'xxx0064.jpg', 'xxx0065.jpg', 'xxx0066.jpg', 'xxx0067.jpg', 'xxx0068.jpg', 'xxx0069.jpg', 'xxx0070.jpg', 'xxx0071.jpg', 'xxx0072.jpg', 'xxx0073.jpg', 'xxx0074.jpg', 'xxx0075.jpg', 'xxx0076.jpg', 'xxx0077.jpg', 'xxx0078.jpg', 'xxx0079.jpg', 'xxx0080.jpg', 'xxx0081.jpg', 'xxx0082.jpg', 'xxx0083.jpg', 'xxx0084.jpg', 'xxx0085.jpg', 'xxx0086.jpg', 'xxx0087.jpg', 'xxx0088.jpg', 'xxx0089.jpg', 'xxx0090.jpg', 'xxx0091.jpg', 'xxx0092.jpg', 'xxx0093.jpg', 'xxx0094.jpg', 'xxx0095.jpg', 'xxx0096.jpg', 'xxx0097.jpg', 'xxx0098.jpg', 'xxx0099.jpg', 'xxx0100.jpg', 'xxx0101.jpg', 'xxx0102.jpg', 'xxx0103.jpg', 'xxx0104.jpg', 'xxx0105.jpg', 'xxx0106.jpg', 'xxx0107.jpg', 'xxx0108.jpg', 'xxx0109.jpg', 'xxx0110.jpg', 'xxx0111.jpg', 'xxx0112.jpg', 'xxx0113.jpg', 'xxx0114.jpg', 'xxx0115.jpg', 'xxx0116.jpg', 'xxx0117.jpg', 'xxx0118.jpg', 'xxx0119.jpg', 'xxx0120.jpg', 'xxx0121.jpg', 'xxx0122.jpg', 'xxx0123.jpg', 'xxx0124.jpg', 'xxx0125.jpg', 'xxx0126.jpg', 'xxx0127.jpg', 'xxx0128.jpg', 'xxx0129.jpg', 'xxx0130.jpg', 'xxx0131.jpg', 'xxx0132.jpg', 'xxx0133.jpg', 'xxx0134.jpg', 'xxx0135.jpg', 'xxx0136.jpg', 'xxx0137.jpg', 'xxx0138.jpg', 'xxx0139.jpg', 'xxx0140.jpg', 'xxx0141.jpg', 'xxx0142.jpg', 'xxx0143.jpg', 'xxx0144.jpg', 'xxx0145.jpg', 'xxx0146.jpg', 'xxx0147.jpg', 'xxx0148.jpg', 'xxx0149.jpg', 'xxx0150.jpg', 'xxx0151.jpg', 'xxx0152.jpg', 'xxx0153.jpg', 'xxx0154.jpg', 'xxx0155.jpg', 'xxx0156.jpg', 'xxx0157.jpg', 'xxx0158.jpg', 'xxx0159.jpg', 'xxx0160.jpg', 'xxx0161.jpg', 'xxx0162.jpg', 'xxx0163.jpg', 'xxx0164.jpg', 'xxx0165.jpg', 'xxx0166.jpg', 'xxx0167.jpg', 'xxx0168.jpg', 'xxx0169.jpg', 'xxx0170.jpg', 'xxx0171.jpg', 'xxx0172.jpg', 'xxx0173.jpg', 'xxx0174.jpg', 'xxx0175.jpg', 'xxx0176.jpg', 'xxx0177.jpg', 'xxx0178.jpg', 'xxx0179.jpg', 'xxx0180.jpg', 'xxx0181.jpg', 'xxx0182.jpg', 'xxx0183.jpg', 'xxx0184.jpg', 'xxx0185.jpg', 'xxx0186.jpg', 'xxx0187.jpg', 'xxx0188.jpg', 'xxx0189.jpg', 'xxx0190.jpg', 'xxx0191.jpg', 'xxx0192.jpg', 'xxx0193.jpg', 'xxx0194.jpg', 'xxx0195.jpg', 'xxx0196.jpg', 'xxx0197.jpg', 'xxx0198.jpg', 'xxx0199.jpg', 'xxx0200.jpg', 'xxx0201.jpg', 'xxx0202.jpg', 'xxx0203.jpg', 'xxx0204.jpg', 'xxx0205.jpg', 'xxx0206.jpg', 'xxx0207.jpg', 'xxx0208.jpg', 'xxx0209.jpg', 'xxx0210.jpg', 'xxx0211.jpg', 'xxx0212.jpg', 'xxx0213.jpg', 'xxx0214.jpg', 'xxx0215.jpg', 'xxx0216.jpg', 'xxx0217.jpg', 'xxx0218.jpg', 'xxx0219.jpg', 'xxx0220.jpg', 'xxx0221.jpg', 'xxx0222.jpg', 'xxx0223.jpg', 'xxx0224.jpg', 'xxx0225.jpg', 'xxx0226.jpg']\n",
            "226\n",
            "226\n",
            "(226, 545)\n",
            "Feature Extration Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8szFrlheG3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08686315-2905-4cf3-9f43-1a5bdda7998e"
      },
      "source": [
        "# Best model path \n",
        "final_model = \"/drive/MyDrive/CADx project /Challenges/cad/cad2/output2/base_models/base_model_17.pkl\"\n",
        "\n",
        "## Load the best model and predict test data on the model \n",
        "loaded_model = pickle.load(open(final_model, 'rb'))\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "#Look at the predictions and change from name labels to designated labels \n",
        "print(y_pred)     \n",
        "y_pred = np.where(y_pred=='bcc', 0, y_pred)\n",
        "y_pred = np.where(y_pred=='bkl',1,y_pred)\n",
        "y_pred = np.where(y_pred=='mel',2,y_pred)\n",
        "print(y_pred)\n",
        "\n",
        "# Saving NumPy array as a csv file\n",
        "pd.DataFrame(y_pred).to_csv(\"/drive/MyDrive/CADx project /Challenges/cad/cad2/output2/challenge2_predictions.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bkl' 'mel' 'mel' 'bkl' 'mel' 'mel' 'bkl' 'mel' 'mel' 'mel' 'mel' 'bkl'\n",
            " 'bkl' 'bkl' 'mel' 'mel' 'mel' 'mel' 'mel' 'bkl' 'mel' 'mel' 'mel' 'mel'\n",
            " 'bkl' 'mel' 'bkl' 'bkl' 'mel' 'bkl' 'bkl' 'mel' 'bkl' 'mel' 'mel' 'mel'\n",
            " 'mel' 'bkl' 'bcc' 'mel' 'bcc' 'bcc' 'mel' 'bcc' 'bcc' 'mel' 'bkl' 'mel'\n",
            " 'mel' 'mel' 'mel' 'bcc' 'bkl' 'bcc' 'mel' 'mel' 'bcc' 'mel' 'mel' 'mel'\n",
            " 'mel' 'bkl' 'bkl' 'bkl' 'bcc' 'mel' 'mel' 'bkl' 'mel' 'mel' 'bkl' 'mel'\n",
            " 'mel' 'bkl' 'bkl' 'mel' 'bkl' 'bkl' 'bkl' 'bkl' 'mel' 'bkl' 'mel' 'mel'\n",
            " 'bcc' 'mel' 'bkl' 'bkl' 'mel' 'mel' 'bcc' 'mel' 'mel' 'bkl' 'bkl' 'mel'\n",
            " 'mel' 'mel' 'mel' 'bkl' 'mel' 'bkl' 'bkl' 'mel' 'bkl' 'mel' 'bkl' 'mel'\n",
            " 'bkl' 'bkl' 'mel' 'bcc' 'bkl' 'bkl' 'mel' 'bkl' 'bkl' 'bkl' 'bkl' 'bkl'\n",
            " 'bkl' 'bkl' 'bcc' 'bkl' 'bkl' 'bcc' 'bcc' 'bkl' 'mel' 'bkl' 'bkl' 'mel'\n",
            " 'mel' 'mel' 'bkl' 'mel' 'bkl' 'mel' 'mel' 'mel' 'bkl' 'mel' 'mel' 'bkl'\n",
            " 'bkl' 'bkl' 'bkl' 'mel' 'bkl' 'mel' 'bkl' 'mel' 'bkl' 'mel' 'mel' 'bkl'\n",
            " 'mel' 'mel' 'bkl' 'mel' 'mel' 'mel' 'mel' 'bkl' 'mel' 'mel' 'mel' 'bkl'\n",
            " 'bkl' 'bkl' 'bkl' 'mel' 'bkl' 'bkl' 'bkl' 'bcc' 'mel' 'mel' 'bkl' 'bkl'\n",
            " 'mel' 'bkl' 'mel' 'mel' 'bkl' 'mel' 'bkl' 'bcc' 'mel' 'bkl' 'mel' 'mel'\n",
            " 'mel' 'bkl' 'bkl' 'mel' 'mel' 'bkl' 'bkl' 'bkl' 'bkl' 'bcc' 'mel' 'bkl'\n",
            " 'mel' 'bkl' 'bkl' 'mel' 'bkl' 'mel' 'mel' 'mel' 'bkl' 'mel' 'mel' 'bkl'\n",
            " 'mel' 'mel' 'mel' 'mel' 'mel' 'mel' 'bkl' 'bkl' 'mel' 'mel']\n",
            "['1' '2' '2' '1' '2' '2' '1' '2' '2' '2' '2' '1' '1' '1' '2' '2' '2' '2'\n",
            " '2' '1' '2' '2' '2' '2' '1' '2' '1' '1' '2' '1' '1' '2' '1' '2' '2' '2'\n",
            " '2' '1' '0' '2' '0' '0' '2' '0' '0' '2' '1' '2' '2' '2' '2' '0' '1' '0'\n",
            " '2' '2' '0' '2' '2' '2' '2' '1' '1' '1' '0' '2' '2' '1' '2' '2' '1' '2'\n",
            " '2' '1' '1' '2' '1' '1' '1' '1' '2' '1' '2' '2' '0' '2' '1' '1' '2' '2'\n",
            " '0' '2' '2' '1' '1' '2' '2' '2' '2' '1' '2' '1' '1' '2' '1' '2' '1' '2'\n",
            " '1' '1' '2' '0' '1' '1' '2' '1' '1' '1' '1' '1' '1' '1' '0' '1' '1' '0'\n",
            " '0' '1' '2' '1' '1' '2' '2' '2' '1' '2' '1' '2' '2' '2' '1' '2' '2' '1'\n",
            " '1' '1' '1' '2' '1' '2' '1' '2' '1' '2' '2' '1' '2' '2' '1' '2' '2' '2'\n",
            " '2' '1' '2' '2' '2' '1' '1' '1' '1' '2' '1' '1' '1' '0' '2' '2' '1' '1'\n",
            " '2' '1' '2' '2' '1' '2' '1' '0' '2' '1' '2' '2' '2' '1' '1' '2' '2' '1'\n",
            " '1' '1' '1' '0' '2' '1' '2' '1' '1' '2' '1' '2' '2' '2' '1' '2' '2' '1'\n",
            " '2' '2' '2' '2' '2' '2' '1' '1' '2' '2']\n"
          ]
        }
      ]
    }
  ]
}
